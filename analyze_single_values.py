#!/usr/bin/env python3
"""
åˆ†ææ¸…ç†å ±å‘Šä¸­çš„ single_value æ¨¡å¼
"""
import json
import pandas as pd

def analyze_single_values(report_file, csv_file, num_samples=5):
    """
    åˆ†æ single_value æ¨¡å¼çš„æ™‚æˆ³å’Œå‘¨åœæ•¸æ“š
    
    Args:
        report_file: æ¸…ç†å ±å‘Š JSON æª”æ¡ˆè·¯å¾‘
        csv_file: åŸå§‹ CSV æª”æ¡ˆè·¯å¾‘
        num_samples: è¦åˆ†æçš„æ¨£æœ¬æ•¸é‡
    """
    # è®€å–æ¸…ç†å ±å‘Š
    with open(report_file, 'r', encoding='utf-8') as f:
        report = json.load(f)
    
    # è®€å–åŸå§‹ CSV æ•¸æ“š
    df = pd.read_csv(csv_file)
    
    # æ‰¾å‡ºæ‰€æœ‰ single_value çš„ç¾¤é›†
    single_value_clusters = [
        cluster for cluster in report['cluster_analysis'] 
        if cluster['pattern_type'] == 'single_value'
    ]
    
    print(f"ğŸ“Š ç™¼ç¾ {len(single_value_clusters)} å€‹ single_value ç¾¤é›†")
    print(f"ğŸ” åˆ†æå‰ {num_samples} å€‹æ¨£æœ¬:\n")
    
    # åˆ†æå‰ num_samples å€‹
    for i, cluster in enumerate(single_value_clusters[:num_samples]):
        timestamp = cluster['start_timestamp']
        value = cluster['sum_before_cleaning']
        
        print(f"=== æ¨£æœ¬ {i+1} ===")
        print(f"æ™‚æˆ³: {timestamp:.3f} ç§’")
        print(f"ä½ç§»å€¼: {value:.6f} mm")
        print(f"å‹•ä½œ: {cluster['action']}")
        
        # åœ¨ CSV ä¸­æ‰¾åˆ°å°æ‡‰çš„è¡Œ
        time_col = df.columns[0]  # ç¬¬ä¸€æ¬„æ˜¯æ™‚é–“
        disp_col = df.columns[1]  # ç¬¬äºŒæ¬„æ˜¯ä½ç§»
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ™‚æˆ³ç´¢å¼•
        time_diff = abs(df[time_col] - timestamp)
        closest_idx = time_diff.idxmin()
        
        # é¡¯ç¤ºå‘¨åœçš„ä¸Šä¸‹æ–‡ï¼ˆå‰å¾Œå„3è¡Œï¼‰
        start_idx = max(0, closest_idx - 3)
        end_idx = min(len(df), closest_idx + 4)
        
        print(f"å‘¨åœæ•¸æ“š (è¡Œ {start_idx+2} åˆ° {end_idx+1}):")
        print("   è¡Œè™Ÿ  |   æ™‚æˆ³    |    ä½ç§»å€¼     | æ¨™è¨˜")
        print("-" * 45)
        
        for idx in range(start_idx, end_idx):
            row_num = idx + 2  # CSV è¡Œè™Ÿï¼ˆå¾2é–‹å§‹ï¼Œå› ç‚ºæœ‰æ¨™é¡Œè¡Œï¼‰
            time_val = df.iloc[idx][time_col]
            disp_val = df.iloc[idx][disp_col]
            
            # æ¨™è¨˜ç›®æ¨™è¡Œ
            marker = " â† ç›®æ¨™" if idx == closest_idx else ""
            print(f"  {row_num:5d}  | {time_val:8.3f} | {disp_val:12.6f} {marker}")
        
        print()
    
    # çµ±è¨ˆ single_value çš„åˆ†ä½ˆ
    print("ğŸ“ˆ Single Value çµ±è¨ˆåˆ†æ:")
    values = [cluster['sum_before_cleaning'] for cluster in single_value_clusters]
    
    print(f"  ç¸½æ•¸é‡: {len(values)}")
    print(f"  æœ€å°å€¼: {min(values):.6f} mm")
    print(f"  æœ€å¤§å€¼: {max(values):.6f} mm")
    print(f"  å¹³å‡å€¼: {sum(values)/len(values):.6f} mm")
    
    # åˆ†æä¿ç•™ vs ç§»é™¤çš„æ¯”ä¾‹
    retained = sum(1 for cluster in single_value_clusters if cluster['action'] == 'retained_as_valid')
    removed = len(single_value_clusters) - retained
    
    print(f"  ä¿ç•™: {retained} å€‹ ({retained/len(single_value_clusters)*100:.1f}%)")
    print(f"  ç§»é™¤: {removed} å€‹ ({removed/len(single_value_clusters)*100:.1f}%)")
    
    # é¡¯ç¤ºå€¼çš„åˆ†ä½ˆ
    print(f"\nğŸ“Š æ•¸å€¼åˆ†ä½ˆ:")
    unique_values = {}
    for val in values:
        rounded_val = round(val, 6)
        unique_values[rounded_val] = unique_values.get(rounded_val, 0) + 1
    
    # é¡¯ç¤ºæœ€å¸¸è¦‹çš„å€¼
    sorted_values = sorted(unique_values.items(), key=lambda x: x[1], reverse=True)
    print("  æœ€å¸¸è¦‹çš„ single_value æ•¸å€¼:")
    for val, count in sorted_values[:10]:  # é¡¯ç¤ºå‰10å€‹
        print(f"    {val:12.6f} mm: {count:3d} æ¬¡ ({count/len(values)*100:.1f}%)")

def main():
    """ä¸»å‡½æ•¸"""
    report_file = "lifts/result/1_cleaning_report.json"
    csv_file = "lifts/result/1.csv"
    
    print("ğŸ” Single Value æ¨¡å¼åˆ†æå·¥å…·")
    print("=" * 50)
    
    try:
        analyze_single_values(report_file, csv_file, num_samples=5)
    except FileNotFoundError as e:
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {e}")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")

if __name__ == '__main__':
    main()
