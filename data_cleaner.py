#!/usr/bin/env python3
"""
æ•¸æ“šæ¸…ç†è…³æœ¬ - ç§»é™¤CSVæª”æ¡ˆä¸­çš„ç•«é¢æŠ–å‹•é›œè¨Š
"""
import os
import csv
import json
import numpy as np
from pathlib import Path
from datetime import datetime

class DataCleaner:
    def __init__(self):
        self.noise_threshold_multiplier = 2.0  # é›œè¨Šé–¾å€¼å€æ•¸ï¼Œæ”¹ç‚º2.0å€æ›´åš´æ ¼
        self.isolation_threshold_seconds = 1.0  # é›¢ç¾¤å€¼æ™‚é–“é–¾å€¼ï¼ˆç§’ï¼‰
    
    def detect_noise_threshold(self, values):
        """
        æª¢æ¸¬æ¯å€‹æª”æ¡ˆçš„é›œè¨Šé–¾å€¼
        åŸºæ–¼æœ€å°éé›¶çµ•å°å€¼ä¾†åˆ¤æ–·åŸºæœ¬ç•«ç´ éœ‡ç›ªå¤§å°
        è€ƒæ…®è¨ˆç®—èª¤å·®çš„å®¹å¿åº¦
        """
        values = np.array(values)
        non_zero = values[values != 0.0]
        
        if len(non_zero) == 0:
            return 0.0
        
        # æ‰¾åˆ°æœ€å°çš„éé›¶çµ•å°å€¼ä½œç‚ºåŸºæœ¬é›œè¨Šå–®ä½
        abs_values = np.abs(non_zero)
        min_abs_value = np.min(abs_values)
        
        # åˆ†ææ‰€æœ‰éé›¶å€¼ï¼Œæ‰¾å‡ºåŸºæœ¬å–®ä½çš„è¿‘ä¼¼å€¼
        # å…è¨±10%çš„è¨ˆç®—èª¤å·®å®¹å¿åº¦
        tolerance = 0.1
        basic_unit = min_abs_value
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å…¶ä»–å€¼æ˜¯åŸºæœ¬å–®ä½çš„å€æ•¸ï¼ˆè€ƒæ…®èª¤å·®ï¼‰
        possible_multiples = []
        for val in abs_values:
            ratio = val / basic_unit
            # æª¢æŸ¥æ˜¯å¦æ¥è¿‘æ•´æ•¸å€
            rounded_ratio = round(ratio)
            if abs(ratio - rounded_ratio) <= tolerance and rounded_ratio <= 5:
                possible_multiples.append(rounded_ratio)
        
        # å¦‚æœå¤§éƒ¨åˆ†å€¼éƒ½æ˜¯åŸºæœ¬å–®ä½çš„æ•´æ•¸å€ï¼Œç¢ºèªåŸºæœ¬å–®ä½
        if len(set(possible_multiples)) > 1:
            # æœ‰å¤šç¨®å€æ•¸ï¼ŒåŸºæœ¬å–®ä½æ‡‰è©²æ­£ç¢º
            pass
        else:
            # å¯èƒ½éœ€è¦é‡æ–°èª¿æ•´åŸºæœ¬å–®ä½
            # å°‹æ‰¾æœ€å¤§å…¬ç´„æ•¸çš„è¿‘ä¼¼å€¼
            sorted_values = np.sort(abs_values)
            if len(sorted_values) > 1:
                # ä½¿ç”¨æœ€å°çš„å…©å€‹å€¼ä¾†æ¨ä¼°åŸºæœ¬å–®ä½
                basic_unit = min(sorted_values[0], sorted_values[1] / round(sorted_values[1] / sorted_values[0]))
        
        # é›œè¨Šé–¾å€¼è¨­ç‚ºåŸºæœ¬å–®ä½çš„å€æ•¸ï¼ŒåŠ ä¸Šå®¹å¿åº¦
        noise_threshold = basic_unit * (self.noise_threshold_multiplier + tolerance)
        
        return noise_threshold
    
    def is_noise_pattern(self, window, noise_threshold):
        """
        åˆ¤æ–·ä¸€å€‹çª—å£æ˜¯å¦ç‚ºé›œè¨Šæ¨¡å¼
        æ¢ä»¶ï¼š
        1. çª—å£å…§æœ‰éé›¶å€¼
        2. çª—å£å…§æ‰€æœ‰å€¼çš„çµ•å°å€¼éƒ½å°æ–¼é›œè¨Šé–¾å€¼
        3. çª—å£çš„ç¸½å’Œæ¥è¿‘é›¶ï¼ˆæ­£è² ç›¸æ¶ˆï¼‰
        4. è€ƒæ…®è¨ˆç®—èª¤å·®çš„å®¹å¿åº¦
        """
        window = np.array(window)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éé›¶å€¼
        if np.all(window == 0):
            return False
        
        # æª¢æŸ¥æ‰€æœ‰éé›¶å€¼æ˜¯å¦éƒ½åœ¨é›œè¨Šé–¾å€¼å…§
        non_zero_mask = window != 0
        if np.any(np.abs(window[non_zero_mask]) > noise_threshold):
            return False
        
        # æª¢æŸ¥çª—å£ç¸½å’Œæ˜¯å¦æ¥è¿‘é›¶ï¼ˆç›¸å°æ–¼é›œè¨Šé–¾å€¼ï¼‰
        # æ”¾å¯¬å®¹å¿åº¦ä»¥è™•ç†è¨ˆç®—èª¤å·®
        window_sum = np.sum(window)
        tolerance_factor = 0.8  # å…è¨±æ›´å¤§çš„ç¸½å’Œåå·®
        if abs(window_sum) > noise_threshold * tolerance_factor:
            return False
        
        # é¡å¤–æª¢æŸ¥ï¼šå¦‚æœçª—å£å…§æœ‰æ­£è² å€¼äº¤æ›¿æ¨¡å¼ï¼Œæ›´å¯èƒ½æ˜¯é›œè¨Š
        non_zero_values = window[non_zero_mask]
        if len(non_zero_values) >= 2:
            # æª¢æŸ¥æ˜¯å¦æœ‰æ­£è² å€¼
            has_positive = np.any(non_zero_values > 0)
            has_negative = np.any(non_zero_values < 0)
            
            if has_positive and has_negative:
                # æœ‰æ­£è² å€¼ï¼Œæ›´å¯èƒ½æ˜¯é›œè¨Šï¼Œé™ä½é–¾å€¼è¦æ±‚
                return abs(window_sum) <= noise_threshold * 1.2
        
        return True
    
    def clean_data(self, values, timestamps=None):
        """
        æ¸…ç†æ•¸æ“šä¸­çš„é›œè¨Š - æ–°çš„å…©éšæ®µæ–¹æ³•
        éšæ®µä¸€ï¼šWindow Size 2 æ­£è² å°æª¢æ¸¬
        éšæ®µäºŒï¼šé›¢ç¾¤å–®ç¨å€¼æª¢æ¸¬
        è¿”å›æ¸…ç†å¾Œçš„æ•¸æ“šå’Œè©³ç´°çš„è™•ç†è¨˜éŒ„
        """
        values = np.array(values, dtype=float)
        original_values = values.copy()
        cleaned_values = values.copy()
        
        if timestamps is None:
            timestamps = list(range(len(values)))
        
        # æª¢æ¸¬é›œè¨Šé–¾å€¼å’ŒåŸºæœ¬å–®ä½
        noise_threshold = self.detect_noise_threshold(values)
        basic_unit = self._get_basic_unit(values)
        
        # åˆå§‹åŒ–è™•ç†è¨˜éŒ„
        processing_details = []
        cluster_analysis = []
        cluster_id = 0
        
        # éšæ®µçµ±è¨ˆ
        stage1_stats = {"pairs_detected": 0, "points_removed": 0, "positive_negative": 0, "negative_positive": 0}
        stage2_stats = {"candidates_checked": 0, "points_removed": 0}
        
        if noise_threshold == 0:
            # æ²’æœ‰éé›¶å€¼ï¼Œè¨˜éŒ„æ‰€æœ‰é›¶å€¼
            for i, (timestamp, value) in enumerate(zip(timestamps, values)):
                if value == 0:
                    processing_details.append({
                        "timestamp": timestamp,
                        "original_value": value,
                        "action": "retained",
                        "stage": "final",
                        "reason": "zero_value",
                        "cluster_id": None
                    })
            return cleaned_values, 0, noise_threshold, basic_unit, processing_details, cluster_analysis, stage1_stats, stage2_stats
        
        # === éšæ®µä¸€ï¼šWindow Size 2 æ­£è² å°æª¢æ¸¬ ===
        processed_indices = set()
        i = 0
        while i <= len(cleaned_values) - 2:
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
            if i in processed_indices or (i + 1) in processed_indices:
                i += 1
                continue
            
            window = cleaned_values[i:i+2]
            window_timestamps = timestamps[i:i+2]
            
            if self._is_noise_pair(window, noise_threshold):
                cluster_id += 1
                stage1_stats["pairs_detected"] += 1
                stage1_stats["points_removed"] += 2
                
                # åˆ¤æ–·æ˜¯æ­£è² å°é‚„æ˜¯è² æ­£å°
                if window[0] > 0 and window[1] < 0:
                    pair_type = "positive_negative"
                    stage1_stats["positive_negative"] += 1
                else:
                    pair_type = "negative_positive"
                    stage1_stats["negative_positive"] += 1
                
                # è¨˜éŒ„ç¾¤é›†åˆ†æ
                cluster_analysis.append({
                    "cluster_id": cluster_id,
                    "stage": "stage_1_pair",
                    "pattern_type": f"{pair_type}_pair",
                    "start_timestamp": window_timestamps[0],
                    "end_timestamp": window_timestamps[1],
                    "total_values": 2,
                    "sum_before_cleaning": float(np.sum(window)),
                    "action": "removed_as_noise"
                })
                
                # è¨˜éŒ„æ¯å€‹å€¼çš„è™•ç†è©³æƒ…
                for j, (timestamp, original_val) in enumerate(zip(window_timestamps, window)):
                    processing_details.append({
                        "timestamp": timestamp,
                        "original_value": original_val,
                        "action": "removed",
                        "stage": "stage_1_pair",
                        "reason": f"{pair_type}_pair",
                        "pair_partner_timestamp": window_timestamps[1-j],  # é…å°å¤¥ä¼´çš„æ™‚é–“æˆ³
                        "cluster_id": cluster_id
                    })
                    processed_indices.add(i + j)
                
                # å°‡é…å°è¨­ç‚º0
                cleaned_values[i:i+2] = 0.0
                i += 2  # è·³éå·²è™•ç†çš„é…å°
            else:
                i += 1
        
        # === éšæ®µäºŒï¼šé›¢ç¾¤å–®ç¨å€¼æª¢æ¸¬ ===
        for i, (timestamp, original_val, cleaned_val) in enumerate(zip(timestamps, original_values, cleaned_values)):
            if i not in processed_indices and cleaned_val != 0:
                stage2_stats["candidates_checked"] += 1
                
                if self._is_isolated_noise(i, timestamps, cleaned_values, basic_unit):
                    cluster_id += 1
                    stage2_stats["points_removed"] += 1
                    
                    # è¨˜éŒ„ç¾¤é›†åˆ†æ
                    cluster_analysis.append({
                        "cluster_id": cluster_id,
                        "stage": "stage_2_isolated",
                        "pattern_type": "isolated_small_value",
                        "start_timestamp": timestamp,
                        "end_timestamp": timestamp,
                        "total_values": 1,
                        "sum_before_cleaning": float(cleaned_val),
                        "action": "removed_as_noise"
                    })
                    
                    # è¨˜éŒ„è™•ç†è©³æƒ…
                    processing_details.append({
                        "timestamp": timestamp,
                        "original_value": original_val,
                        "action": "removed",
                        "stage": "stage_2_isolated",
                        "reason": "isolated_small_value",
                        "cluster_id": cluster_id
                    })
                    
                    # ç§»é™¤é›¢ç¾¤å€¼
                    cleaned_values[i] = 0.0
                    processed_indices.add(i)
        
        # === è¨˜éŒ„ä¿ç•™çš„å€¼ ===
        for i, (timestamp, original_val, cleaned_val) in enumerate(zip(timestamps, original_values, cleaned_values)):
            if i not in processed_indices and original_val != 0:
                if cleaned_val != 0:  # è¢«ä¿ç•™çš„å€¼
                    cluster_id += 1
                    
                    # è¨˜éŒ„ç¾¤é›†åˆ†æ
                    cluster_analysis.append({
                        "cluster_id": cluster_id,
                        "stage": "final",
                        "pattern_type": "valid_movement",
                        "start_timestamp": timestamp,
                        "end_timestamp": timestamp,
                        "total_values": 1,
                        "sum_before_cleaning": float(original_val),
                        "action": "retained_as_valid"
                    })
                    
                    # è¨˜éŒ„è™•ç†è©³æƒ…
                    processing_details.append({
                        "timestamp": timestamp,
                        "original_value": original_val,
                        "action": "retained",
                        "stage": "final",
                        "reason": "valid_movement",
                        "cluster_id": cluster_id
                    })
        
        # è¨ˆç®—ç§»é™¤çš„é›œè¨Šé»æ•¸
        original_non_zero = np.sum(original_values != 0)
        cleaned_non_zero = np.sum(cleaned_values != 0)
        total_noise_removed = original_non_zero - cleaned_non_zero
        
        return cleaned_values, total_noise_removed, noise_threshold, basic_unit, processing_details, cluster_analysis, stage1_stats, stage2_stats
    
    def _get_basic_unit(self, values):
        """ç²å–åŸºæœ¬é›œè¨Šå–®ä½"""
        values = np.array(values)
        non_zero = values[values != 0.0]
        
        if len(non_zero) == 0:
            return 0.0
        
        abs_values = np.abs(non_zero)
        return float(np.min(abs_values))
    
    def _is_noise_pair(self, window, noise_threshold):
        """
        åˆ¤æ–·ä¸€å€‹å¤§å°ç‚º2çš„çª—å£æ˜¯å¦ç‚ºé›œè¨Šé…å°
        æ¢ä»¶ï¼š
        1. å…©å€‹å€¼éƒ½éé›¶
        2. å…©å€‹å€¼ç¬¦è™Ÿç›¸åï¼ˆä¸€æ­£ä¸€è² ï¼‰
        3. å…©å€‹å€¼çš„çµ•å°å€¼éƒ½å°æ–¼é›œè¨Šé–¾å€¼
        4. å…©å€‹å€¼çš„ç¸½å’Œæ¥è¿‘é›¶ï¼ˆæ­£è² ç›¸æ¶ˆï¼‰
        """
        window = np.array(window)
        
        # æª¢æŸ¥æ˜¯å¦å…©å€‹å€¼éƒ½éé›¶
        if np.any(window == 0):
            return False
        
        # æª¢æŸ¥æ˜¯å¦ç¬¦è™Ÿç›¸å
        if window[0] * window[1] >= 0:  # åŒè™Ÿæˆ–å…¶ä¸­ä¸€å€‹ç‚º0
            return False
        
        # æª¢æŸ¥å…©å€‹å€¼çš„çµ•å°å€¼æ˜¯å¦éƒ½åœ¨é›œè¨Šé–¾å€¼å…§
        if np.any(np.abs(window) > noise_threshold):
            return False
        
        # æª¢æŸ¥ç¸½å’Œæ˜¯å¦æ¥è¿‘é›¶ï¼ˆæ­£è² ç›¸æ¶ˆï¼‰
        window_sum = np.sum(window)
        tolerance_factor = 0.3  # å…è¨±30%çš„åå·®
        return abs(window_sum) <= noise_threshold * tolerance_factor
    
    def _is_isolated_noise(self, index, timestamps, values, basic_unit):
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºé›¢ç¾¤é›œè¨Š
        æ¢ä»¶ï¼š
        1. å€¼ç‚º1-2.1å€åŸºæœ¬å–®ä½å¤§å°ï¼ˆåŒ…å«10%å®¹å¿åº¦ï¼‰
        2. èˆ‡æœ€è¿‘çš„éé›¶å€¼ç›¸éš”è¶…é isolation_threshold_seconds ç§’
        """
        current_value = values[index]
        current_time = timestamps[index]
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå°å€¼ï¼ˆ1-2.1å€åŸºæœ¬å–®ä½ï¼Œèˆ‡éšæ®µä¸€ä¿æŒä¸€è‡´ï¼‰
        tolerance = 0.1  # 10%å®¹å¿åº¦
        max_threshold = basic_unit * (self.noise_threshold_multiplier + tolerance)
        if abs(current_value) > max_threshold:
            return False
        
        # æ‰¾å°‹æœ€è¿‘çš„éé›¶å€¼çš„æ™‚é–“è·é›¢
        min_distance = float('inf')
        for i, val in enumerate(values):
            if i != index and val != 0:
                distance = abs(timestamps[i] - current_time)
                min_distance = min(min_distance, distance)
        
        # å¦‚æœæ²’æœ‰å…¶ä»–éé›¶å€¼ï¼Œä¹Ÿç®—æ˜¯é›¢ç¾¤
        if min_distance == float('inf'):
            return True
        
        return min_distance > self.isolation_threshold_seconds
    
    def _classify_pattern(self, window):
        """åˆ†é¡é›œè¨Šæ¨¡å¼"""
        window = np.array(window)
        non_zero = window[window != 0]
        
        if len(non_zero) == 0:
            return "all_zero"
        elif len(non_zero) == 1:
            return "single_value"
        elif len(non_zero) == 2:
            if non_zero[0] * non_zero[1] < 0:  # ç›¸åç¬¦è™Ÿ
                return "alternating_pair"
            else:
                return "same_sign_pair"
        elif len(non_zero) == 4:
            signs = np.sign(non_zero)
            if np.array_equal(signs, [1, -1, -1, 1]) or np.array_equal(signs, [-1, 1, 1, -1]):
                return "four_alternating_pattern"
            elif np.array_equal(signs, [1, -1, 1, -1]) or np.array_equal(signs, [-1, 1, -1, 1]):
                return "strict_alternating_pattern"
            else:
                return "four_value_pattern"
        else:
            return f"{len(non_zero)}_value_pattern"
    
    def process_csv_file(self, input_path, output_path):
        """
        è™•ç†å–®å€‹CSVæª”æ¡ˆä¸¦ç”Ÿæˆè©³ç´°å ±å‘Š
        """
        times = []
        values = []
        
        # è®€å–æ•¸æ“š
        with open(input_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)  # è®€å–æ¨™é¡Œè¡Œ
            
            for row in reader:
                if len(row) >= 2:
                    try:
                        time_val = float(row[0])
                        displacement_val = float(row[1])
                        times.append(time_val)
                        values.append(displacement_val)
                    except ValueError:
                        continue
        
        # æ¸…ç†æ•¸æ“šä¸¦ç²å–è©³ç´°è¨˜éŒ„
        cleaned_values, noise_count, threshold, basic_unit, processing_details, cluster_analysis, stage1_stats, stage2_stats = self.clean_data(values, times)
        
        # å¯«å…¥æ¸…ç†å¾Œçš„æ•¸æ“š
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(header)  # å¯«å…¥æ¨™é¡Œè¡Œ
            
            for time_val, clean_val in zip(times, cleaned_values):
                writer.writerow([time_val, clean_val])
        
        # ç”Ÿæˆæ¸…ç†å ±å‘Š
        report = self._generate_cleaning_report(
            input_path, output_path, times, values, cleaned_values,
            basic_unit, threshold, processing_details, cluster_analysis, stage1_stats, stage2_stats
        )
        
        # ä¿å­˜å ±å‘Š
        report_path = self._save_cleaning_report(input_path, report)
        
        return {
            'original_points': len(values),
            'noise_points_removed': noise_count,
            'noise_threshold': threshold,
            'non_zero_original': np.sum(np.array(values) != 0),
            'non_zero_cleaned': np.sum(cleaned_values != 0),
            'report_path': report_path
        }
    
    def _generate_cleaning_report(self, input_path, output_path, times, original_values, cleaned_values,
                                 basic_unit, threshold, processing_details, cluster_analysis, stage1_stats, stage2_stats):
        """ç”Ÿæˆæ¸…ç†å ±å‘Š - æ–°çš„å…©éšæ®µæ ¼å¼"""
        input_file = Path(input_path).name
        output_file = Path(output_path).name
        
        # æ’åºè™•ç†è©³æƒ…
        processing_details.sort(key=lambda x: x['timestamp'])
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        original_non_zero = np.sum(np.array(original_values) != 0)
        cleaned_non_zero = np.sum(cleaned_values != 0)
        total_removed = original_non_zero - cleaned_non_zero
        
        # è¨ˆç®—ç§»é™¤ç‡
        total_removal_rate = (total_removed / original_non_zero * 100) if original_non_zero > 0 else 0
        stage1_removal_rate = (stage1_stats["points_removed"] / original_non_zero * 100) if original_non_zero > 0 else 0
        stage2_removal_rate = (stage2_stats["points_removed"] / original_non_zero * 100) if original_non_zero > 0 else 0
        
        report = {
            "file_info": {
                "original_file": input_file,
                "cleaned_file": output_file,
                "processing_time": datetime.now().isoformat(),
                "cleaner_version": "2.0"  # æ–°ç‰ˆæœ¬
            },
            "detection_parameters": {
                "basic_unit_mm": float(basic_unit),
                "noise_threshold_mm": float(threshold),
                "threshold_multiplier": float(self.noise_threshold_multiplier),
                "isolation_threshold_seconds": float(self.isolation_threshold_seconds),
                "tolerance_factor": 0.1
            },
            "processing_stages": {
                "stage_1_pair_detection": {
                    "description": "Window Size 2 æ­£è² å°æª¢æ¸¬",
                    "pairs_detected": stage1_stats["pairs_detected"],
                    "points_removed": stage1_stats["points_removed"],
                    "pattern_types": {
                        "positive_negative": stage1_stats["positive_negative"],
                        "negative_positive": stage1_stats["negative_positive"]
                    }
                },
                "stage_2_isolated_detection": {
                    "description": "é›¢ç¾¤å–®ç¨å€¼æª¢æ¸¬",
                    "candidates_checked": stage2_stats["candidates_checked"],
                    "points_removed": stage2_stats["points_removed"],
                    "isolation_criteria": {
                        "min_distance_seconds": float(self.isolation_threshold_seconds),
                        "max_value_multiplier": 2.1  # 2.0 + 10%å®¹å¿åº¦
                    }
                }
            },
            "summary_stats": {
                "total_data_points": len(original_values),
                "original_non_zero_points": int(original_non_zero),
                "stage_1_removed": int(stage1_stats["points_removed"]),
                "stage_2_removed": int(stage2_stats["points_removed"]),
                "total_removed": int(total_removed),
                "final_non_zero_points": int(cleaned_non_zero),
                "total_removal_rate": round(total_removal_rate, 1),
                "stage_1_removal_rate": round(stage1_removal_rate, 1),
                "stage_2_removal_rate": round(stage2_removal_rate, 1)
            },
            "processing_details": processing_details,
            "cluster_analysis": cluster_analysis
        }
        
        return report
    
    def _save_cleaning_report(self, input_path, report):
        """ä¿å­˜æ¸…ç†å ±å‘Šåˆ°JSONæª”æ¡ˆ"""
        input_path = Path(input_path)
        
        # ç”Ÿæˆå ±å‘Šæª”åï¼šåŸæª”å_cleaning_report.json
        report_filename = f"{input_path.stem}_cleaning_report.json"
        report_path = input_path.parent / report_filename
        
        # ä¿å­˜ç‚ºæ ¼å¼åŒ–çš„JSON
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return str(report_path)
    
    def process_directory(self, input_dir, output_dir=None):
        """
        è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰CSVæª”æ¡ˆ
        """
        input_path = Path(input_dir)
        
        if output_dir is None:
            output_path = input_path
        else:
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
        
        # åªè™•ç†åŸå§‹æª”æ¡ˆï¼Œä¸è™•ç†å·²æ¸…ç†çš„æª”æ¡ˆï¼ˆé¿å…é‡è¤‡è™•ç†c*.csvæª”æ¡ˆï¼‰
        csv_files = [f for f in input_path.glob("*.csv") if not f.name.startswith('c')]
        
        if not csv_files:
            print(f"åœ¨ {input_dir} ä¸­æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆ")
            return
        
        print(f"æ‰¾åˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆ\n")
        
        total_stats = {
            'files_processed': 0,
            'total_noise_removed': 0,
            'total_original_points': 0
        }
        
        for csv_file in csv_files:
            # ç”Ÿæˆè¼¸å‡ºæª”åï¼ˆåŠ ä¸Š 'c' å‰ç¶´ï¼‰
            output_filename = f"c{csv_file.name}"
            output_file_path = output_path / output_filename
            
            print(f"è™•ç†: {csv_file.name} -> {output_filename}")
            
            try:
                stats = self.process_csv_file(csv_file, output_file_path)
                
                print(f"  åŸå§‹æ•¸æ“šé»: {stats['original_points']}")
                print(f"  åŸå§‹éé›¶é»: {stats['non_zero_original']}")
                print(f"  æ¸…ç†å¾Œéé›¶é»: {stats['non_zero_cleaned']}")
                print(f"  ç¸½ç§»é™¤é›œè¨Šé»: {stats['noise_points_removed']}")
                print(f"  é›œè¨Šé–¾å€¼: {stats['noise_threshold']:.6f} mm")
                
                # åˆ†éšæ®µçµ±è¨ˆï¼ˆå¾å ±å‘Šä¸­è®€å–ï¼‰
                report_path = stats['report_path']
                try:
                    with open(report_path, 'r', encoding='utf-8') as f:
                        report_data = json.load(f)
                    
                    stage1_removed = report_data['summary_stats']['stage_1_removed']
                    stage2_removed = report_data['summary_stats']['stage_2_removed']
                    pairs_detected = report_data['processing_stages']['stage_1_pair_detection']['pairs_detected']
                    
                    print(f"  ğŸ“Š éšæ®µä¸€ (æ­£è² å°): æª¢æ¸¬åˆ° {pairs_detected} å°, ç§»é™¤ {stage1_removed} é»")
                    print(f"  ğŸ“Š éšæ®µäºŒ (é›¢ç¾¤å€¼): ç§»é™¤ {stage2_removed} é»")
                    
                    # å®‰å…¨è¨ˆç®—é›œè¨Šç§»é™¤ç‡
                    if stats['non_zero_original'] > 0:
                        total_rate = stats['noise_points_removed']/stats['non_zero_original']*100
                        stage1_rate = stage1_removed/stats['non_zero_original']*100
                        stage2_rate = stage2_removed/stats['non_zero_original']*100
                        print(f"  ğŸ“ˆ ç¸½ç§»é™¤ç‡: {total_rate:.1f}% (éšæ®µä¸€: {stage1_rate:.1f}%, éšæ®µäºŒ: {stage2_rate:.1f}%)")
                    else:
                        print(f"  ğŸ“ˆ ç¸½ç§»é™¤ç‡: 0.0%")
                except:
                    # å¦‚æœè®€å–å ±å‘Šå¤±æ•—ï¼Œä½¿ç”¨èˆŠæ ¼å¼
                    if stats['non_zero_original'] > 0:
                        removal_rate = stats['noise_points_removed']/stats['non_zero_original']*100
                        print(f"  é›œè¨Šç§»é™¤ç‡: {removal_rate:.1f}%")
                    else:
                        print(f"  é›œè¨Šç§»é™¤ç‡: 0.0%")
                
                # é¡¯ç¤ºå ±å‘Šè·¯å¾‘
                print(f"  ğŸ“Š æ¸…ç†å ±å‘Š: {Path(stats['report_path']).name}")
                
                total_stats['files_processed'] += 1
                total_stats['total_noise_removed'] += stats['noise_points_removed']
                total_stats['total_original_points'] += stats['original_points']
                
            except Exception as e:
                print(f"  éŒ¯èª¤: {e}")
            
            print()
        
        print("=" * 50)
        print("ç¸½è™•ç†çµ±è¨ˆ:")
        print(f"è™•ç†æª”æ¡ˆæ•¸: {total_stats['files_processed']}")
        print(f"ç¸½æ•¸æ“šé»: {total_stats['total_original_points']}")
        print(f"ç¸½é›œè¨Šç§»é™¤æ•¸: {total_stats['total_noise_removed']}")


def main():
    """
    ä¸»å‡½æ•¸ - è™•ç† lifts/result ç›®éŒ„ä¸­çš„æ‰€æœ‰ CSV æª”æ¡ˆ
    """
    cleaner = DataCleaner()
    
    # è™•ç† lifts/result ç›®éŒ„
    input_directory = "lifts/result"
    
    if not os.path.exists(input_directory):
        print(f"éŒ¯èª¤: ç›®éŒ„ {input_directory} ä¸å­˜åœ¨")
        return
    
    print("æ•¸æ“šæ¸…ç†å·¥å…· - ç§»é™¤ç•«é¢æŠ–å‹•é›œè¨Š")
    print("=" * 50)
    
    cleaner.process_directory(input_directory)
    
    print("æ¸…ç†å®Œæˆï¼")


if __name__ == '__main__':
    main()
