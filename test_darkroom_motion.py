"""
éšæ®µ 1: æš—æˆ¿é‹å‹•åµæ¸¬é©—è­‰è…³æœ¬

ç›®æ¨™: æ¸¬è©¦å®Œæ•´çš„é‹å‹•åµæ¸¬ pipeline åœ¨æš—æˆ¿å€é–“çš„å¯è¡Œæ€§
æ¸¬è©¦ç¯„åœ: 21_a.mp4 çš„ 3:00-3:15 (åŒ…å«é‹å‹•é» 3:05-3:07)
"""
import os
import sys
os.chdir(os.path.dirname(os.path.realpath(__file__)))
sys.path.append("src/")

import cv2
import numpy as np
from scipy.stats import ttest_1samp
from sklearn.cluster import KMeans

# æ¸¬è©¦åƒæ•¸
VIDEO_PATH = "lifts/darkroom_data/21_a.mp4"
TEST_START = 180      # 3:00 (ç§’)
TEST_END = 195        # 3:15 (ç§’)
MOTION_POINT = 185    # 3:05 (é æœŸé‹å‹•é»)
FRAME_INTERVAL = 6    # èˆ‡ä¸»ç¨‹å¼ä¸€è‡´
ROI_RATIO = 0.6       # èˆ‡ä¸»ç¨‹å¼ä¸€è‡´
MIN_MATCHES = 6       # æœ€ä½åŒ¹é…å°æ•¸

# è¼¸å‡ºç›®éŒ„
OUTPUT_DIR = "lifts/darkroom_motion_test"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def preprocess_darkroom_frame(frame):
    """
    æš—æˆ¿å½±ç‰‡å°ˆç”¨å‰è™•ç† - CLAHE å¢å¼·

    Args:
        frame: BGR å½©è‰²å½±åƒ

    Returns:
        enhanced: CLAHE å¢å¼·å¾Œçš„ç°éšå½±åƒ
    """
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    enhanced = clahe.apply(gray)
    return enhanced

def remove_outliers(data, method='iqr'):
    """ç§»é™¤é›¢ç¾¤å€¼ï¼ˆèˆ‡ä¸»ç¨‹å¼é‚è¼¯ä¸€è‡´ï¼‰"""
    if len(data) < 3:
        return np.arange(len(data))

    q1, q3 = np.percentile(data, [25, 75])
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    mask = (data >= lower_bound) & (data <= upper_bound)
    return np.where(mask)[0]

def test_motion_detection():
    """åŸ·è¡Œé‹å‹•åµæ¸¬æ¸¬è©¦"""

    print("ğŸŒ™ æš—æˆ¿é‹å‹•åµæ¸¬é©—è­‰æ¸¬è©¦")
    print("="*70)

    # é–‹å•Ÿå½±ç‰‡
    cap = cv2.VideoCapture(VIDEO_PATH)
    if not cap.isOpened():
        print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {VIDEO_PATH}")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    print(f"ğŸ“¹ å½±ç‰‡è³‡è¨Š:")
    print(f"  FPS: {fps:.2f}")
    print(f"  è§£æåº¦: {width}x{height}")
    print(f"  æ¸¬è©¦å€é–“: {TEST_START}s - {TEST_END}s ({TEST_END-TEST_START}ç§’)")
    print(f"  å¹€é–“éš”: æ¯ {FRAME_INTERVAL} å¹€å–æ¨£")

    # å»ºç«‹ ROI é®ç½©
    mask = np.zeros((height, width), dtype=np.uint8)
    roi_y1 = int(height * ROI_RATIO / 2)
    roi_y2 = int(height * (1 - ROI_RATIO / 2))
    roi_x1 = int(width * ROI_RATIO / 2)
    roi_x2 = int(width * (1 - ROI_RATIO / 2))
    mask[roi_y1:roi_y2, roi_x1:roi_x2] = 1

    print(f"  ROI ç¯„åœ: ({roi_x1}, {roi_y1}) - ({roi_x2}, {roi_y2})")

    # åˆå§‹åŒ–åµæ¸¬å™¨
    orb = cv2.ORB.create(nfeatures=100)
    bf_matcher = cv2.BFMatcher(normType=cv2.NORM_HAMMING, crossCheck=True)
    kmeans = KMeans(n_clusters=2, random_state=0)

    # è·³åˆ°æ¸¬è©¦èµ·é»
    start_frame = int(TEST_START * fps)
    end_frame = int(TEST_END * fps)
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

    # è®€å–ç¬¬ä¸€å¹€
    ret, frame1 = cap.read()
    if not ret:
        print("âŒ ç„¡æ³•è®€å–èµ·å§‹å¹€")
        return

    # å‰è™•ç† + ç‰¹å¾µåµæ¸¬
    enhanced1 = preprocess_darkroom_frame(frame1)
    kp1, desc1 = orb.detectAndCompute(enhanced1, mask)

    print(f"\nâœ… åˆå§‹å¹€ç‰¹å¾µé»: {len(kp1)} å€‹")

    # å„²å­˜ç¬¬ä¸€å¹€çš„è¦–è¦ºåŒ–
    vis_frame1 = frame1.copy()
    cv2.drawKeypoints(vis_frame1, kp1, vis_frame1, color=(0, 255, 0), flags=0)
    cv2.rectangle(vis_frame1, (roi_x1, roi_y1), (roi_x2, roi_y2), (255, 0, 0), 2)
    cv2.imwrite(os.path.join(OUTPUT_DIR, "frame1_features.jpg"), vis_frame1)

    # æ¸¬è©¦çµæœå®¹å™¨
    results = []

    print(f"\n{'='*70}")
    print(f"{'Frame':>6} {'Time':>7} {'KP1':>5} {'KP2':>5} {'Matches':>8} {'V-Travel':>10} {'P-value':>10} {'Status':>10}")
    print(f"{'='*70}")

    frame_idx = start_frame

    while True:
        # è®€å–ä¸‹ä¸€å¹€
        for _ in range(FRAME_INTERVAL):
            ret, frame2 = cap.read()
            frame_idx += 1

        if not ret or frame_idx >= end_frame:
            break

        time_sec = frame_idx / fps

        # å‰è™•ç† + ç‰¹å¾µåµæ¸¬
        enhanced2 = preprocess_darkroom_frame(frame2)
        kp2, desc2 = orb.detectAndCompute(enhanced2, mask)

        # åˆå§‹åŒ–çµæœ
        vertical_travel = 0
        camera_pan = False
        num_matches = 0
        pvalue = 1.0
        status = "no_feature"

        # ç‰¹å¾µåŒ¹é…
        if desc1 is not None and desc2 is not None and len(kp1) > 0 and len(kp2) > 0:
            matches = bf_matcher.match(desc2, desc1)
            num_matches = len(matches)

            if num_matches >= MIN_MATCHES:
                # å»ºç«‹é…å°é™£åˆ—
                paired_info = []
                for m in matches:
                    kp1_coord = np.array(kp1[m.trainIdx].pt)
                    kp2_coord = np.array(kp2[m.queryIdx].pt)

                    distance = np.linalg.norm(kp1_coord - kp2_coord)
                    h_travel = kp2_coord[0] - kp1_coord[0]
                    v_travel = kp2_coord[1] - kp1_coord[1]

                    paired_info.append([m.queryIdx, m.trainIdx, distance, h_travel, v_travel])

                paired_info = np.array(paired_info)

                # ç§»é™¤é›¢ç¾¤å€¼
                valid_idx = remove_outliers(paired_info[:, 2])
                paired_info = paired_info[valid_idx]

                if len(paired_info) >= MIN_MATCHES:
                    # æª¢æŸ¥ camera pan
                    h_travels = paired_info[:, 3]
                    pvalue = ttest_1samp(h_travels, 0).pvalue
                    camera_pan = pvalue < 0.001

                    if not camera_pan:
                        # K-means åˆ†ç¾¤
                        distances = paired_info[:, 2].reshape(-1, 1)
                        if len(set(distances.flatten())) > 1:
                            labels = kmeans.fit_predict(distances)

                            # å…©ç¾¤çš„å‚ç›´ä½ç§»
                            group0_v = paired_info[labels == 0, 4]
                            group1_v = paired_info[labels == 1, 4]

                            pvalue0 = ttest_1samp(group0_v, 0).pvalue
                            pvalue1 = ttest_1samp(group1_v, 0).pvalue

                            v0 = np.median(group0_v) if pvalue0 < 0.0001 else 0
                            v1 = np.median(group1_v) if pvalue1 < 0.0001 else 0

                            if abs(v0) > abs(v1):
                                vertical_travel = int(v1 - v0)
                            else:
                                vertical_travel = int(v0 - v1)

                            status = "motion" if vertical_travel != 0 else "static"
                            pvalue = min(pvalue0, pvalue1)
                        else:
                            status = "no_cluster"
                    else:
                        status = "camera_pan"
                else:
                    status = "few_matches"
            else:
                status = "few_matches"

        # è¨˜éŒ„çµæœ
        results.append({
            'frame_idx': frame_idx,
            'time': time_sec,
            'kp1': len(kp1),
            'kp2': len(kp2),
            'matches': num_matches,
            'vertical_travel': vertical_travel,
            'pvalue': pvalue,
            'camera_pan': camera_pan,
            'status': status
        })

        # è¼¸å‡ºæ—¥èªŒ
        status_icon = "âœ…" if status == "motion" else ("âš ï¸" if status == "camera_pan" else "")
        print(f"{frame_idx:6d} {time_sec:7.2f} {len(kp1):5d} {len(kp2):5d} {num_matches:8d} {vertical_travel:10d} {pvalue:10.4f} {status_icon}{status:>9}")

        # å„²å­˜é—œéµå¹€çš„è¦–è¦ºåŒ–
        if abs(time_sec - MOTION_POINT) < 3:  # é‹å‹•é»é™„è¿‘ Â±3 ç§’
            vis_frame = frame2.copy()

            # ç¹ªè£½ç‰¹å¾µé»
            cv2.drawKeypoints(vis_frame, kp2, vis_frame, color=(0, 255, 0), flags=0)

            # ç¹ªè£½åŒ¹é…ç·š
            if desc1 is not None and desc2 is not None:
                matches = bf_matcher.match(desc2, desc1)
                for m in matches[:20]:  # åªç¹ªè£½å‰ 20 æ¢
                    pt1 = tuple(map(int, kp1[m.trainIdx].pt))
                    pt2 = tuple(map(int, kp2[m.queryIdx].pt))
                    cv2.line(vis_frame, pt1, pt2, (0, 0, 255), 1)

            # ç¹ªè£½ ROI
            cv2.rectangle(vis_frame, (roi_x1, roi_y1), (roi_x2, roi_y2), (255, 0, 0), 2)

            # ç¹ªè£½è³‡è¨Š
            info_text = f"Frame {frame_idx} | {time_sec:.1f}s | {status}"
            cv2.putText(vis_frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
                       0.7, (255, 255, 255), 2)

            travel_text = f"V-Travel: {vertical_travel} px | Matches: {num_matches}"
            cv2.putText(vis_frame, travel_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX,
                       0.7, (255, 255, 255), 2)

            filename = f"frame_{frame_idx}_{time_sec:.1f}s_{status}.jpg"
            cv2.imwrite(os.path.join(OUTPUT_DIR, filename), vis_frame)

        # æ›´æ–°å‰ä¸€å¹€
        kp1, desc1 = kp2, desc2

    cap.release()

    # çµ±è¨ˆåˆ†æ
    print(f"\n{'='*70}")
    print("ğŸ“Š æ¸¬è©¦çµæœçµ±è¨ˆ")
    print(f"{'='*70}")

    total_frames = len(results)
    motion_frames = sum(1 for r in results if r['status'] == 'motion')
    static_frames = sum(1 for r in results if r['status'] == 'static')
    pan_frames = sum(1 for r in results if r['camera_pan'])
    failed_frames = sum(1 for r in results if r['status'] in ['no_feature', 'few_matches', 'no_cluster'])

    print(f"\nç¸½å¹€æ•¸: {total_frames}")
    print(f"  âœ… åµæ¸¬åˆ°é‹å‹•: {motion_frames} å¹€ ({motion_frames/total_frames*100:.1f}%)")
    print(f"  âšª éœæ­¢ç‹€æ…‹:   {static_frames} å¹€ ({static_frames/total_frames*100:.1f}%)")
    print(f"  âš ï¸  Camera Pan: {pan_frames} å¹€ ({pan_frames/total_frames*100:.1f}%)")
    print(f"  âŒ åµæ¸¬å¤±æ•—:   {failed_frames} å¹€ ({failed_frames/total_frames*100:.1f}%)")

    # ç‰¹å¾µé»çµ±è¨ˆ
    avg_kp = np.mean([r['kp2'] for r in results])
    avg_matches = np.mean([r['matches'] for r in results])

    print(f"\nç‰¹å¾µé»çµ±è¨ˆ:")
    print(f"  å¹³å‡ç‰¹å¾µé»æ•¸: {avg_kp:.1f} å€‹")
    print(f"  å¹³å‡åŒ¹é…å°æ•¸: {avg_matches:.1f} å°")

    # é‹å‹•çµ±è¨ˆ
    motion_results = [r for r in results if r['status'] == 'motion']
    if motion_results:
        total_travel = sum(abs(r['vertical_travel']) for r in motion_results)
        print(f"\né‹å‹•çµ±è¨ˆ:")
        print(f"  åµæ¸¬åˆ°é‹å‹•çš„å¹€æ•¸: {len(motion_results)}")
        print(f"  ç¸½å‚ç›´ä½ç§»: {total_travel} åƒç´ ")
        print(f"  å¹³å‡ä½ç§»/å¹€: {total_travel/len(motion_results):.2f} åƒç´ ")

    # æˆåŠŸç‡è©•ä¼°
    success_rate = (motion_frames + static_frames) / total_frames * 100

    print(f"\n{'='*70}")
    print("ğŸ¯ å¯è¡Œæ€§è©•ä¼°")
    print(f"{'='*70}")
    print(f"\nåµæ¸¬æˆåŠŸç‡: {success_rate:.1f}%")

    if success_rate >= 70:
        print("\nâœ… è©•ä¼°çµæœ: åšæ³• Aï¼ˆè‡ªå‹•åŒ–åµæ¸¬ï¼‰å®Œå…¨å¯è¡Œ")
        print("   å»ºè­°: é€²å…¥éšæ®µ 2ï¼Œé–‹ç™¼å®Œæ•´çš„æš—æˆ¿å°ˆç”¨ä¸»ç¨‹å¼")
    elif success_rate >= 50:
        print("\nâš ï¸  è©•ä¼°çµæœ: åšæ³• A å¯è¡Œä½†éœ€è¦èª¿æ•´")
        print("   å»ºè­°: èª¿æ•´åƒæ•¸å¾Œé‡æ–°æ¸¬è©¦ï¼Œæˆ–è€ƒæ…®æ··åˆæ–¹æ¡ˆ")
    else:
        print("\nâŒ è©•ä¼°çµæœ: åšæ³• A ä¸å¯è¡Œ")
        print("   å»ºè­°: æ¡ç”¨åšæ³• Bï¼ˆäººå·¥è¼”åŠ©åµæ¸¬ï¼‰æˆ–æ··åˆæ–¹æ¡ˆ")

    print(f"\nè¦–è¦ºåŒ–çµæœå·²å„²å­˜è‡³: {OUTPUT_DIR}/")

    # å„²å­˜è©³ç´°å ±å‘Š
    report_path = os.path.join(OUTPUT_DIR, "test_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("æš—æˆ¿é‹å‹•åµæ¸¬é©—è­‰å ±å‘Š\n")
        f.write("="*70 + "\n\n")
        f.write(f"æ¸¬è©¦å½±ç‰‡: {VIDEO_PATH}\n")
        f.write(f"æ¸¬è©¦å€é–“: {TEST_START}s - {TEST_END}s\n")
        f.write(f"ç¸½å¹€æ•¸: {total_frames}\n\n")
        f.write("è©³ç´°çµæœ:\n")
        f.write("-"*70 + "\n")
        for r in results:
            f.write(f"Frame {r['frame_idx']:6d} | {r['time']:7.2f}s | "
                   f"KP: {r['kp2']:3d} | Matches: {r['matches']:3d} | "
                   f"V-Travel: {r['vertical_travel']:5d} px | "
                   f"Status: {r['status']}\n")
        f.write("\n" + "="*70 + "\n")
        f.write(f"æˆåŠŸç‡: {success_rate:.1f}%\n")

    print(f"è©³ç´°å ±å‘Šå·²å„²å­˜è‡³: {report_path}")

if __name__ == "__main__":
    test_motion_detection()